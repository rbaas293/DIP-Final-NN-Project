{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"c:\\python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"c:\\python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"c:\\python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n  File \"c:\\python36\\lib\\imp.py\", line 243, in load_module\n    return load_dynamic(name, filename, file)\n  File \"c:\\python36\\lib\\imp.py\", line 343, in load_dynamic\n    return _load(spec)\nImportError: DLL load failed: The specified module could not be found.\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m   \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m   \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_mod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0m_pywrap_tensorflow_internal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mswig_import_helper\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m                 \u001b[0m_mod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_pywrap_tensorflow_internal'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpathname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\imp.py\u001b[0m in \u001b[0;36mload_module\u001b[1;34m(name, file, filename, details)\u001b[0m\n\u001b[0;32m    242\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 243\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mload_dynamic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    244\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mtype_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mPKG_DIRECTORY\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\imp.py\u001b[0m in \u001b[0;36mload_dynamic\u001b[1;34m(name, path, file)\u001b[0m\n\u001b[0;32m    342\u001b[0m             name=name, loader=loader, origin=path)\n\u001b[1;32m--> 343\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed: The specified module could not be found.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-30e4b00ff319>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# pylint: disable=g-bad-import-order\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m  \u001b[1;31m# pylint: disable=unused-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;31m# Protocol buffers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msome\u001b[0m \u001b[0mcommon\u001b[0m \u001b[0mreasons\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msolutions\u001b[0m\u001b[1;33m.\u001b[0m  \u001b[0mInclude\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mentire\u001b[0m \u001b[0mstack\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m above this error message when asking for help.\"\"\" % traceback.format_exc()\n\u001b[1;32m---> 74\u001b[1;33m   \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;31m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"c:\\python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"c:\\python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"c:\\python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n  File \"c:\\python36\\lib\\imp.py\", line 243, in load_module\n    return load_dynamic(name, filename, file)\n  File \"c:\\python36\\lib\\imp.py\", line 343, in load_dynamic\n    return _load(spec)\nImportError: DLL load failed: The specified module could not be found.\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help."
     ]
    }
   ],
   "source": [
    "#Import Things\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import pathlib\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enable eager ... cant do this with nightly builds for -gpu ... also must have version 10.0 of cuda toolkit\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "#################################################################################\n",
    "#################### DOWLOAD A SET OF JPG IMAGES ################################\n",
    "#################################################################################\n",
    "\"\"\"Import Stuff Created In This Section:\n",
    "\n",
    "    data_root: a Path variable describing where a shit load of jpg images are stored\n",
    "    all_image_paths: a list of paths that refer to a shit load of jpg images\n",
    "    all_image_labels: a list of the categories that correspond to each of the images in all_image_paths\n",
    "    my_attributions: a dictionary of things that describe the jpg images\n",
    "    label_to_index: a dictionary of possible categories for these images\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download the file\n",
    "data_root_orig = tf.keras.utils.get_file(\n",
    "    origin='https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n",
    "    fname='flower_photos', untar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the path where the files were saved\n",
    "data_root = pathlib.Path(data_root_orig)\n",
    "print(\"JPG images downloaded to the following location: \", data_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate a list of all the files we just downloaded\n",
    "all_image_paths = list(data_root.glob('*/*'))\n",
    "all_image_paths = [str(path) for path in all_image_paths]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle the list of jpg paths\n",
    "random.shuffle(all_image_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the LICENSE.txt file, which contains all the info we need to know about the images.\n",
    "my_attributions = (data_root / \"LICENSE.txt\").open(encoding='utf-8').readlines()[4:]\n",
    "my_attributions = [line.split(' CC-BY') for line in my_attributions]\n",
    "my_attributions = dict(my_attributions)\n",
    "\n",
    "\"\"\"This function parses out a caption for an image given its path and the overall attribtuions dict\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run it\n",
    "def caption_image(image_path, attributions):\n",
    "    # added as_posix() to this line, because windows path doesn't match LICENSE.txt format.\n",
    "    image_rel = pathlib.Path(image_path).relative_to(data_root).as_posix()\n",
    "    return \"Image (CC BY 2.0) \" + ' - '.join(attributions[str(image_rel)].split(' - ')[:-1])\n",
    "\n",
    "\n",
    "# #\n",
    "# # for n in range(3):\n",
    "# #   image_path = random.choice(all_image_paths)\n",
    "# #   display.display(display.Image(image_path))\n",
    "# #   print(caption_image(image_path))\n",
    "# #   print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#determine the text for image label possibilities, then assign numbers to those options in a dict\n",
    "label_names = sorted(item.name for item in data_root.glob('*/') if item.is_dir())\n",
    "label_to_index = dict((name, index) for index, name in enumerate(label_names))\n",
    "print(\"The label_to_index dict is: \", label_to_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list of each images label, corresponding to all_image_paths\n",
    "all_image_labels = [label_to_index[pathlib.Path(path).parent.name]\n",
    "                    for path in all_image_paths]\n",
    "\n",
    "#################################################################################\n",
    "#################### GENERATE A SET OF WATERMARKED IMAGES #######################\n",
    "#################################################################################\n",
    "\"\"\"Import Stuff Created In This Section:\n",
    "\n",
    "    watermarked_root: a Path variable describing where a shit load of WATERMARKED jpg images are stored\n",
    "    all_watermarked_paths: a list of paths that refer to a shit load of WATERMARKED jpg images\n",
    "    \n",
    "    The following variables still correspond to all_watermarked_paths:\n",
    "    all_image_labels: a list of the categories that correspond to each of the images in all_image_paths\n",
    "    my_attributions: a dictionary of things that describe the jpg images\n",
    "    label_to_index: a dictionary of possible categories for these images\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import watermaking paths to variables\n",
    "pwd = os.path.dirname(os.path.realpath(__file__)) + \"//\"\n",
    "FONT_PATH = pwd + \"current_test_data//BERNHC.TTF\"\n",
    "WATERMARK_TEXT = \"SHEETZ\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Watermark a picture!\n",
    "def watermark_text(input_image_path, output_image_path, text, pos):\n",
    "    photo = Image.open(input_image_path)\n",
    "\n",
    "    # make the image editable\n",
    "    drawing = ImageDraw.Draw(photo)\n",
    "\n",
    "    black = (3, 8, 12)\n",
    "    font = ImageFont.truetype(FONT_PATH, size=50)\n",
    "    drawing.text(pos, text, fill=black, font=font)\n",
    "    # photo.show()\n",
    "    photo.save(output_image_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new directory path string to house the watermarked images (does not actually create the dir)\n",
    "watermarked_root = data_root.with_name(str(data_root.name)+\"_watermarked\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#makes a blank list for strings indicating paths to watermarked photos, in the same order as all_image_paths\n",
    "all_watermarked_paths = []\n",
    "\n",
    "print(\"Creating Watermarked Directory....\")\n",
    "for img_path in all_image_paths:\n",
    "    p = pathlib.Path(img_path)\n",
    "    output_path = watermarked_root/p.parent.name/p.name\n",
    "    # watermark_text(\n",
    "        # img_path,\n",
    "        # output_path,\n",
    "        # text=WATERMARK_TEXT,\n",
    "        # pos=(0, 0)\n",
    "    # )\n",
    "    all_watermarked_paths += [str(output_path)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "#################### CONVERT FROM JPG TO DATASET ################################\n",
    "#################################################################################\n",
    "\n",
    "def load_and_preprocess_image(img_path):\n",
    "    img_raw = tf.io.read_file(img_path)\n",
    "    img_tensor=preprocess_image(img_raw)\n",
    "    return img_tensor\n",
    "\n",
    "\n",
    "def preprocess_image(image):\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, [192,192])\n",
    "    image /= 255.0\n",
    "    return image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Below unpacks The tuples into the positional arguments of the mapped function\n",
    "def load_and_preprocess_from_path_label(path, label):\n",
    "  return load_and_preprocess_image(path), label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set counter to zero and build from the list baby!\n",
    "counter = 0\n",
    "\n",
    "def build_from_list(paths, labels, counter):\n",
    "    # Build a tf.data.Dataset\n",
    "\n",
    "    print(\"\\n\\nBuilding A Dataset...\", counter)\n",
    "    counter = counter+1\n",
    "\n",
    "    path_ds = tf.data.Dataset.from_tensor_slices(paths)\n",
    "    image_ds = path_ds.map(load_and_preprocess_image, num_parallel_calls=AUTOTUNE)\n",
    "    label_ds = tf.data.Dataset.from_tensor_slices(labels)\n",
    "    ds = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
    "\n",
    "    image_label_ds = ds.map(load_and_preprocess_from_path_label)\n",
    "    print(image_label_ds)\n",
    "\n",
    "    BATCH_SIZE = 32\n",
    "\n",
    "    # Setting a shuffle buffer size as large as the dataset ensures that the data is\n",
    "    # completely shuffled.\n",
    "    ds = image_label_ds.shuffle(buffer_size=len(paths))\n",
    "    ds = ds.repeat()\n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "    # `prefetch` lets the dataset fetch batches, in the background while the model is training.\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "    ds = image_label_ds.apply(\n",
    "      tf.data.experimental.shuffle_and_repeat(buffer_size=len(labels)))\n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "    print(ds)\n",
    "\n",
    "    # The dataset may take a few seconds to start, as it fills its shuffle buffer.\n",
    "    image_batch, label_batch = next(iter(ds))\n",
    "\n",
    "    return image_batch, label_batch\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train thing setup 1\n",
    "train_og_paths = all_image_paths[:5]\n",
    "train_og_labels = all_image_labels[:5]\n",
    "test_og_paths = all_image_paths[5:]\n",
    "test_og_labels = all_image_labels[5:]\n",
    "train_wm_paths = all_watermarked_paths[:5]\n",
    "train_wm_labels = all_image_labels[:5]\n",
    "test_wm_paths = all_watermarked_paths[5:]\n",
    "test_wm_labels = all_image_labels[5:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train thing setup 2\n",
    "train_og_image_batch, train_og_label_batch = build_from_list(\n",
    "    train_og_paths, train_og_labels, counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train thing setup 3\n",
    "test_og_image_batch, test_og_label_batch = build_from_list(\n",
    "    test_og_paths, test_og_labels, counter)\n",
    "\n",
    "#train_wm_image_batch, train_wm_label_batch = build_from_list(\n",
    "#   train_wm_paths, train_wm_labels, counter)\n",
    "\n",
    "#test_wm_image_batch, test_wm_label_batch = build_from_list(\n",
    "#    test_wm_paths, test_wm_labels, counter)\n",
    "\n",
    "#################################################################################\n",
    "################### END OF FILE #################################################\n",
    "#################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale the 0-255 values down to 0-1 before feeding them to the model\n",
    "print(\"\\n\\n DEBUG 1\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the model -- setup the layers\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(192, 192, 3)),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "print(\"\\n\\n DEBUG 2\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the model -- compile the model!!\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(\"\\n\\n DEBUG 3\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the Model Finally!!!\n",
    "model.fit(train_og_image_batch, train_og_label_batch)\n",
    "\n",
    "\n",
    "#######################################\n",
    "########## TEST MODEL #################\n",
    "#######################################\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_loss, test_acc = model.evaluate(test_og_image_batch, test_og_label_batch)\n"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
